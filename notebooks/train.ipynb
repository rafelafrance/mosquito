{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2cc742-b498-43e2-9d8c-dc9dfb64a9a7",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c9c59a-db53-4ee6-8a0a-34773981678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bcb9c6-e7ad-459e-a465-417ca8878bf5",
   "metadata": {},
   "source": [
    "### Constants derived in segment_tiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8c25ec-d8a3-4e99-b2a4-5b49f1e410be",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "LAYER_DIR = DATA_DIR / \"layers\"\n",
    "\n",
    "STRIPE_CSV = DATA_DIR / \"stripes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6dc6d85-79d7-4a21-b4c7-28afe8db44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abab2c9c-fde7-4110-8f2e-bf104bd4e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [\n",
    "    LAYER_DIR / \"fa.tif\",\n",
    "    LAYER_DIR / \"slope.tif\",\n",
    "    LAYER_DIR / \"wetness.tif\",\n",
    "    LAYER_DIR / \"dem.tif\",\n",
    "]\n",
    "\n",
    "TARGET = LAYER_DIR / \"larv_spot_50m_correct.tif\"\n",
    "\n",
    "STRIPE_CSV = DATA_DIR / \"stripes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c21c22-29c3-4a22-949e-51c8fa509ff1",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d054002-e85b-485e-9c05-b0a031265bcd",
   "metadata": {},
   "source": [
    "A convenience class for working with image stripes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b93b5e7-b9cc-468a-bb6a-766fd341d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Stripe:\n",
    "    dataset: str  # Train, val, test\n",
    "    row: int  # Top pixel of stripe\n",
    "    beg: int  # First column with data\n",
    "    end: int  # Last column with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9694d51c-7c68-40c8-9fc1-d16171396ebb",
   "metadata": {},
   "source": [
    "The dataset class itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "955afef4-efbf-40cd-a972-b35d56a1434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerDataset(Dataset):\n",
    "    def __init__(self, stripe_csv, layers, target=None, augment=False):\n",
    "        self.layers = np.stack([io.imread(lay) for lay in layers], axis=2)\n",
    "        self.target = io.imread(target) if target else None\n",
    "        self.transform = self.build_transforms(augment)\n",
    "\n",
    "        with open(stripe_csv) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            self.stripes = [Stripe(**s) for s in reader]\n",
    "\n",
    "    def __len__(self):\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ...\n",
    "\n",
    "    def build_transforms(model, augment=False):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69943f7d-6bd1-4515-8f65-2e90375ffe7a",
   "metadata": {},
   "source": [
    "### A simple U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ca35f3-c8c0-4778-bc98-07b3a6707125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 4,\n",
    "        out_channels: int = 1,\n",
    "        features: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = self.double_conv(in_channels, features)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.encoder1 = self.block(features, features * 2)\n",
    "        self.encoder2 = self.block(features * 2, features * 4)\n",
    "        self.encoder3 = self.block(features * 4, features * 8)\n",
    "        self.encoder4 = self.block(features * 8, features * 16)\n",
    "\n",
    "        self.bottleneck = nn.conv2d(features * 16, features * 16)\n",
    "\n",
    "        self.unpool4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = self.block((features * 8) * 2, features * 8)\n",
    "\n",
    "        self.unpool3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = self.block((features * 4) * 2, features * 4)\n",
    "\n",
    "        self.unpool2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = self.block((features * 2) * 2, features * 2)\n",
    "\n",
    "        self.unpool1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = self.block(features * 2, features)\n",
    "\n",
    "        self.output = nn.Conv2d(features, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        enc1 = self.pool(self.encoder1(x))\n",
    "        enc2 = self.pool(self.encoder2(enc1))\n",
    "        enc3 = self.pool(self.encoder3(enc2))\n",
    "        enc4 = self.pool(self.encoder4(enc3))\n",
    "\n",
    "        x = self.bottleneck(enc4)\n",
    "\n",
    "        x = self.unpool4(x)\n",
    "        x = self.decoder4(torch.cat(x, enc4, dim=1))\n",
    "\n",
    "        x = self.unpool3(x)\n",
    "        x = self.decoder3(torch.cat(x, enc3, dim=1))\n",
    "\n",
    "        x = self.unpool2(x)\n",
    "        x = self.decoder2(torch.cat(x, enc2, dim=1))\n",
    "\n",
    "        x = self.unpool1(bottleneck)\n",
    "        x = self.decoder1(torch.cat(x, enc1, dim=1))\n",
    "\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c9bbc-19d4-4327-8523-64e18c2293dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "venv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
