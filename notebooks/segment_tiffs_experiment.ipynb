{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0293714e-e549-4f29-84f3-abfe8af8324b",
   "metadata": {},
   "source": [
    "# Segment TIFF images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8db9ff-a639-4eff-83f9-e2c9955313ff",
   "metadata": {},
   "source": [
    "The general idea is to use the large TIFF images as input layers for a U-Net for training, validation and testing. I'll do this by chopping the image layers into smaller chunks and feeding them into the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8007fff5-8010-4f32-876e-8f6e4de2513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34bc793c-5382-4b72-a9e8-fcbe20a54873",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_DIR = Path(\"..\") / \"data\" / \"layers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f06d5-1f93-4dc3-89a9-7364c2e1e49a",
   "metadata": {},
   "source": [
    "## Look at image properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a361c7-8e93-4e8b-8551-374f9af29b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), 0.0, 3.4e+38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa = io.imread(LAYER_DIR / \"fa.tif\")\n",
    "fa.shape, fa.dtype, fa.min(), fa.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf6bcf5-cf55-4c2a-83d9-64400957a76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), -3.4028235e+38, 0.9276394)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope = io.imread(LAYER_DIR / \"slope.tif\")\n",
    "slope.shape, slope.dtype, slope.min(), slope.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d765e119-7eb0-49fe-a87a-4108a15d40a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), -3.4028235e+38, 53.34468)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wetness = io.imread(LAYER_DIR / \"wetness.tif\")\n",
    "wetness.shape, wetness.dtype, wetness.min(), wetness.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89534c18-cb5e-4f5e-a0f9-583690979496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), -3.4028235e+38, 53.149834)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem = io.imread(LAYER_DIR / \"dem.tif\")\n",
    "dem.shape, dem.dtype, dem.min(), dem.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e38910-1dbb-4ebd-8869-a14eea04bd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), 0.0, 3.4e+38)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "larv = io.imread(LAYER_DIR / \"larv_spot_50m_correct.tif\")\n",
    "larv.shape, larv.dtype, larv.min(), larv.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1529b2db-760e-431a-b0ca-8573bbc2ffb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.4028235e+38, 3.4028235e+38)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(np.float32).min, np.finfo(np.float32).max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6872d5-4a75-4d77-b253-f87da78fe113",
   "metadata": {},
   "source": [
    "## How many tiles can we actually use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a38864-47be-4d6b-abac-1a445dd20ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a548c4-ccf9-4857-a338-abd2b975a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81.3828125, 39.025390625, 3175.996047973633)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = fa.shape[0] / tile_size\n",
    "w = fa.shape[1] / tile_size\n",
    "h, w, h * w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0b7b4-81cb-46ca-8a1d-2bc7c1c54a44",
   "metadata": {},
   "source": [
    "These numbers are based off of raw image size but much of the image is masked out, so try a closer appoximation. The unmassked areas are mostly contiguous but offset from row to row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ed5db0-bee2-4cb1-be0a-4f0f54b3a0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278090069, 1060.8294258117676)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_value = (fa < 3.0e38).sum()\n",
    "has_value, has_value / (tile_size * tile_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d405d8a-bec6-4974-adb0-ba010ff1458d",
   "metadata": {},
   "source": [
    "Well that is rather depressing.\n",
    "\n",
    "I need to cordon off areas for the test and valdation datasets. The remainder is for the training dataset. I'll randomly crop the training data. The edges between the 3 datasets are absolute but tiles may partially hang out into the undefined regions.\n",
    "\n",
    "There is a triangle at the top of the images that has no targets. Should I include that? For now, \"Yes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ad22a-93a2-4a02-857d-45fdc4aa009f",
   "metadata": {},
   "source": [
    "Dataset distribution strategy:\n",
    "1. Slice the images into 81 rows of tile sized data.\n",
    "2. Randomly assign the rows to the three datasets.\n",
    "There will be 16 testing and validation stripes and (81 - 32 =) 49 training stripes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc898a2-50ed-4c1b-bdab-e6da3b2f06a2",
   "metadata": {},
   "source": [
    "## Segment the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea174db0-88b1-4ea5-9629-9d99cd5a6f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1c9ea1d-5343-4c56-a145-bf10aa91ea8f",
   "metadata": {},
   "source": [
    "## A U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da56292-6eb9-4743-9b99-fe95d5e5f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 4,\n",
    "        out_channels: int = 1,\n",
    "        features: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = self.double_conv(in_channels, features)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.encoder1 = self.block(features, features * 2)\n",
    "        self.encoder2 = self.block(features * 2, features * 4)\n",
    "        self.encoder3 = self.block(features * 4, features * 8)\n",
    "        self.encoder4 = self.block(features * 8, features * 16)\n",
    "\n",
    "        self.bottleneck = nn.conv2d(features * 16, features * 16)\n",
    "\n",
    "        self.unpool4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = self.block((features * 8) * 2, features * 8)\n",
    "\n",
    "        self.unpool3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = self.block((features * 4) * 2, features * 4)\n",
    "\n",
    "        self.unpool2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = self.block((features * 2) * 2, features * 2)\n",
    "\n",
    "        self.unpool1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = self.block(features * 2, features)\n",
    "\n",
    "        self.output = nn.Conv2d(features, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        enc1 = self.encoder1(self.pool(x))\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "\n",
    "        x = self.bottleneck(enc4)\n",
    "\n",
    "        x = self.unpool4(x)\n",
    "        x = self.decoder4(torch.cat(x, enc4, dim=1))\n",
    "\n",
    "        x = self.unpool3(x)\n",
    "        x = self.decoder3(torch.cat(x, enc3, dim=1))\n",
    "\n",
    "        x = self.unpool2(x)\n",
    "        x = self.decoder2(torch.cat(x, enc2, dim=1))\n",
    "\n",
    "        x = self.unpool1(bottleneck)\n",
    "        x = self.decoder1(torch.cat(x, enc1, dim=1))\n",
    "\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec008f-4673-402b-9067-db5b15f4e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "venv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
