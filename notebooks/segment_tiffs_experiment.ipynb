{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0293714e-e549-4f29-84f3-abfe8af8324b",
   "metadata": {},
   "source": [
    "# Segment TIFF images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8007fff5-8010-4f32-876e-8f6e4de2513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.model_selection as msel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34bc793c-5382-4b72-a9e8-fcbe20a54873",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_DIR = Path(\"..\") / \"data\" / \"layers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f06d5-1f93-4dc3-89a9-7364c2e1e49a",
   "metadata": {},
   "source": [
    "## Look at image properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbe3d3-bedb-4bd7-b547-c097fab0cb58",
   "metadata": {},
   "source": [
    "For each layer print the height, width, data type, minimum, and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a361c7-8e93-4e8b-8551-374f9af29b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), 0.0, 3.4e+38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FA = io.imread(LAYER_DIR / \"fa.tif\")\n",
    "FA.shape, FA.dtype, FA.min(), FA.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf6bcf5-cf55-4c2a-83d9-64400957a76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), -3.4028235e+38, 0.9276394)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLOPE = io.imread(LAYER_DIR / \"slope.tif\")\n",
    "SLOPE.shape, SLOPE.dtype, SLOPE.min(), SLOPE.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d765e119-7eb0-49fe-a87a-4108a15d40a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), -3.4028235e+38, 53.34468)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WETNESS = io.imread(LAYER_DIR / \"wetness.tif\")\n",
    "WETNESS.shape, WETNESS.dtype, WETNESS.min(), WETNESS.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89534c18-cb5e-4f5e-a0f9-583690979496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), -3.4028235e+38, 53.149834)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEM = io.imread(LAYER_DIR / \"dem.tif\")\n",
    "DEM.shape, DEM.dtype, DEM.min(), DEM.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e38910-1dbb-4ebd-8869-a14eea04bd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41668, 19981), dtype('float32'), 0.0, 3.4e+38)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LARV = io.imread(LAYER_DIR / \"larv_spot_50m_correct.tif\")\n",
    "LARV.shape, LARV.dtype, LARV.min(), LARV.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd52b7-0094-460f-8312-e44045761565",
   "metadata": {},
   "source": [
    "It appears that \"Not a Number\" (NaN) values area represented by the largest or smallest float32 value.\n",
    "\n",
    "Let's compare with the true minimum and maximum values for float32. None of the used map values are anywhere near to these extrema which makes them easy to identify and filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1529b2db-760e-431a-b0ca-8573bbc2ffb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.4028235e+38, 3.4028235e+38)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(np.float32).min, np.finfo(np.float32).max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d717d2-2ae0-4b93-bf0a-7c105aa401fb",
   "metadata": {},
   "source": [
    "I'll give myself some wiggle room for the constants that I'll use to detect NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d4264c-4337-4d88-9e4c-87af509d3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_LO = -3.0e38\n",
    "NA_HI = 3.0e38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81a9b8-e296-4a8d-81fd-0c23611a1f53",
   "metadata": {},
   "source": [
    "I'll going to want to exclude tiles that have a majority of NaN values. For now I'll set this threshold to 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6872d5-4a75-4d77-b253-f87da78fe113",
   "metadata": {},
   "source": [
    "## How many tiles can we actually use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287c544-9786-4b2c-8bfc-f2a226eb1c3c",
   "metadata": {},
   "source": [
    "I only have one large image (with 4 layers) for training, validation, and testing. The strategy is to pretend that I've got several images by slicing the large image into several smaller images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b74f1f-97ba-41d4-aa01-5f949a8b98ec",
   "metadata": {},
   "source": [
    "I'll start with an arbitrary tile size of 512 x 512 pixels height & width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a38864-47be-4d6b-abac-1a445dd20ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68aaa3e2-4473-4342-afa6-e7821bc156be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS, COLS = FA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc898a2-50ed-4c1b-bdab-e6da3b2f06a2",
   "metadata": {},
   "source": [
    "## Segment the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ad22a-93a2-4a02-857d-45fdc4aa009f",
   "metadata": {},
   "source": [
    "Dataset distribution strategy:\n",
    "1. Slice the images into 81 rows of tile sized data.\n",
    "2. Randomly assign the rows to the three datasets.\n",
    "3. Keep the sets the same between runs by pinning random state.\n",
    "\n",
    "here is a triangle at the top of the images that has no targets. Should I include that? For now, \"Yes.\"\n",
    "\n",
    "Using a 60/20/20% (train/val/test) split there will be 16 testing and validation stripes and (81 - 32 =) 49 training stripes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88452299-12e4-4e55-8629-b09ed15e4514",
   "metadata": {},
   "source": [
    "#### Put image rows (stripes) into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea174db0-88b1-4ea5-9629-9d99cd5a6f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 49, 16, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_ROWS = list(range(ROWS // TILE_SIZE))\n",
    "\n",
    "TRAIN_INDEXES, others = msel.train_test_split(\n",
    "    ALL_ROWS, train_size=0.61, random_state=4486\n",
    ")\n",
    "VAL_INDEXES, TEST_INDEXES = msel.train_test_split(\n",
    "    others, test_size=0.5, random_state=9241\n",
    ")\n",
    "\n",
    "TRAIN_INDEXES = sorted(TRAIN_INDEXES)\n",
    "VAL_INDEXES = sorted(VAL_INDEXES)\n",
    "TEST_INDEXES = sorted(TEST_INDEXES)\n",
    "\n",
    "len(ALL_ROWS), len(TRAIN_INDEXES), len(VAL_INDEXES), len(TEST_INDEXES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f5b09-3810-4837-9d8b-869868f0e6f1",
   "metadata": {},
   "source": [
    "#### Calculate validation and testing dataset tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b75fe5-4422-4504-99c9-b27919fbde8d",
   "metadata": {},
   "source": [
    "I need to select a static set of tiles for the validation and testing datasets. I'll handle training data separately below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54d1b089-80f9-4445-b9cb-6206cf13e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_LIMIT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d3fb9-7341-4c99-bb91-2d7ff508a481",
   "metadata": {},
   "source": [
    "A class for storing tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd544b17-13c8-4840-ba8b-736614cf54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Tile:\n",
    "    row: int  # Top\n",
    "    col: int  # Left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147cf23-cd75-4db9-9837-19a676f92d32",
   "metadata": {},
   "source": [
    "How many pixels are NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b96896a3-a563-45a2-9575-18fdd9d7b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_data(row, col):\n",
    "    tile = FA[row : row + TILE_SIZE, col : col + TILE_SIZE]\n",
    "    flag = ((tile > NA_LO) & (tile < NA_HI)).any()\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d0c8cf-caeb-4b73-a787-f0f9c53b3abc",
   "metadata": {},
   "source": [
    "Only choose tiles that have enough data tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a28aa45f-d427-405a-8e7e-1944f7f2f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_test_tiles(indexes):\n",
    "    tiles = []\n",
    "    has_blank = False\n",
    "    for i in indexes:\n",
    "        row = i * TILE_SIZE\n",
    "        for col in range(0, COLS, TILE_SIZE):\n",
    "            flag = has_data(row, col)\n",
    "            if flag:\n",
    "                tiles.append(Tile(row, col))\n",
    "            elif not has_blank:\n",
    "                tiles.append(Tile(row, col))\n",
    "                has_blank = True\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e8d4ce2-8476-4a06-b9f7-449d7baffc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "VAL_TILES = val_test_tiles(VAL_INDEXES)\n",
    "print(len(VAL_TILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "107ba001-9f6d-4f32-86e5-4da5a0f2e9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    }
   ],
   "source": [
    "TEST_TILES = val_test_tiles(TEST_INDEXES)\n",
    "print(len(TEST_TILES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8ccfe-85fa-46f1-93a8-3e985f55da7e",
   "metadata": {},
   "source": [
    "#### Organize the training dataset tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f560b-7c9a-4de1-8817-1fb11239f9c6",
   "metadata": {},
   "source": [
    "Group the training rows (stripes) and find all of the possible tiles. Note that there will be further augmentations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e67e7ce-7c60-4dfa-a9c3-51c3ca91b4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 4 5 6 9 10 13 14 16 17 19 21 22 23 26 28 30 31 32 36 37 38 41 42 43 44 45 48 49 52 53 54 57 59 60 62 63 65 66 67 68 69 73 74 75 76 78 79 \n",
      "(0, 1) (3, 7) (9, 11) (13, 15) (16, 18) (19, 20) (21, 24) (26, 27) (28, 29) (30, 33) (36, 39) (41, 46) (48, 50) (52, 55) (57, 58) (59, 61) (62, 64) (65, 70) (73, 77) (78, 80) "
     ]
    }
   ],
   "source": [
    "def join_train_stipes():\n",
    "    row_beg = TRAIN_INDEXES[0]\n",
    "    row_end = row_beg + 1\n",
    "\n",
    "    stripes = []\n",
    "\n",
    "    for i in TRAIN_INDEXES[1:]:\n",
    "        if i == row_end:\n",
    "            row_end = i + 1\n",
    "        else:\n",
    "            stripes.append((row_beg, row_end))\n",
    "            row_beg = i\n",
    "            row_end = i + 1\n",
    "\n",
    "    stripes.append((row_beg, i + 1))\n",
    "    return stripes\n",
    "\n",
    "\n",
    "stripes = join_train_stipes()\n",
    "\n",
    "for i in TRAIN_INDEXES:\n",
    "    print(i, end=\" \")\n",
    "print()\n",
    "\n",
    "for i in stripes:\n",
    "    print(i, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a80fa8b6-243a-45a3-9b88-a7bb0ad37a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [22:56<00:00, 68.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2893665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SKIP = 8\n",
    "\n",
    "\n",
    "def train_tiles(stripes):\n",
    "    tiles = []\n",
    "    count = 0\n",
    "    has_blank = False\n",
    "    for beg, end in tqdm(stripes, position=0):\n",
    "        top = beg * TILE_SIZE\n",
    "        bot = end * TILE_SIZE\n",
    "        for row in range(top, bot, SKIP):\n",
    "            for col in range(0, COLS, SKIP):\n",
    "                flag = has_data(row, col)\n",
    "                if flag:\n",
    "                    tiles.append(Tile(row, col))\n",
    "                elif not has_blank:\n",
    "                    tiles.append(Tile(row, col))\n",
    "                    has_blank = True\n",
    "    return tiles\n",
    "\n",
    "\n",
    "TRAIN_TILES = train_tiles(stripes)\n",
    "print(len(TRAIN_TILES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a776786-6e44-4045-975c-2d3f4ae43694",
   "metadata": {},
   "source": [
    "2.8 million is a wee bit of overkill for now, I'll dial that back in the next round. Data is going to be correlated so the huge number is meaningless and I should expect overfitting. We'll see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930063dd-51a3-4469-a831-d85cc4d4fa82",
   "metadata": {},
   "source": [
    "## Build a data class etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e5bd5-fd51-4a4c-9d42-1baaba7b6c3b",
   "metadata": {},
   "source": [
    "I'll copy code over from other projects & tweak it for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9ea1d-5343-4c56-a145-bf10aa91ea8f",
   "metadata": {},
   "source": [
    "## A U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da56292-6eb9-4743-9b99-fe95d5e5f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 4,\n",
    "        out_channels: int = 1,\n",
    "        features: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = self.double_conv(in_channels, features)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.encoder1 = self.block(features, features * 2)\n",
    "        self.encoder2 = self.block(features * 2, features * 4)\n",
    "        self.encoder3 = self.block(features * 4, features * 8)\n",
    "        self.encoder4 = self.block(features * 8, features * 16)\n",
    "\n",
    "        self.bottleneck = nn.conv2d(features * 16, features * 16)\n",
    "\n",
    "        self.unpool4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = self.block((features * 8) * 2, features * 8)\n",
    "\n",
    "        self.unpool3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = self.block((features * 4) * 2, features * 4)\n",
    "\n",
    "        self.unpool2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = self.block((features * 2) * 2, features * 2)\n",
    "\n",
    "        self.unpool1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = self.block(features * 2, features)\n",
    "\n",
    "        self.output = nn.Conv2d(features, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        enc1 = self.pool(self.encoder1(x))\n",
    "        enc2 = self.pool(self.encoder2(enc1))\n",
    "        enc3 = self.pool(self.encoder3(enc2))\n",
    "        enc4 = self.pool(self.encoder4(enc3))\n",
    "\n",
    "        x = self.bottleneck(enc4)\n",
    "\n",
    "        x = self.unpool4(x)\n",
    "        x = self.decoder4(torch.cat(x, enc4, dim=1))\n",
    "\n",
    "        x = self.unpool3(x)\n",
    "        x = self.decoder3(torch.cat(x, enc3, dim=1))\n",
    "\n",
    "        x = self.unpool2(x)\n",
    "        x = self.decoder2(torch.cat(x, enc2, dim=1))\n",
    "\n",
    "        x = self.unpool1(bottleneck)\n",
    "        x = self.decoder1(torch.cat(x, enc1, dim=1))\n",
    "\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec008f-4673-402b-9067-db5b15f4e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "venv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
